{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189a0603",
   "metadata": {},
   "source": [
    "**In this notebook, we will set up the R square for treeshap. Before that, we will first re-implement treeSHAP using the new framework through complex root of unity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdd674",
   "metadata": {},
   "source": [
    "# Tree Summary\n",
    "Before we start the new algorithm, we introduce some code to retrieve the informationto be used from the tree of other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6f2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005939006805419922\n",
      "tree_summary(children_left=array([  1,   2,   3,   4,   5,   6,  -1,  -1,   9,  -1,  -1,  12,  13,\n",
      "        -1,  -1,  16,  -1,  -1,  19,  20,  21,  -1,  -1,  -1,  25,  26,\n",
      "        -1,  -1,  29,  -1,  -1,  32,  33,  34,  35,  -1,  -1,  38,  -1,\n",
      "        -1,  41,  42,  -1,  -1,  45,  -1,  -1,  48,  49,  50,  -1,  -1,\n",
      "        53,  -1,  -1,  56,  57,  -1,  -1,  -1,  61,  62,  63,  64,  65,\n",
      "        -1,  -1,  68,  -1,  -1,  71,  72,  -1,  -1,  75,  -1,  -1,  78,\n",
      "        79,  80,  -1,  -1,  -1,  84,  85,  -1,  -1,  88,  -1,  -1,  91,\n",
      "        92,  93,  94,  -1,  -1,  97,  -1,  -1, 100, 101,  -1,  -1, 104,\n",
      "        -1,  -1, 107, 108, 109,  -1,  -1, 112,  -1,  -1, 115, 116,  -1,\n",
      "        -1, 119,  -1,  -1], dtype=int64), children_right=array([ 60,  31,  18,  11,   8,   7,  -1,  -1,  10,  -1,  -1,  15,  14,\n",
      "        -1,  -1,  17,  -1,  -1,  24,  23,  22,  -1,  -1,  -1,  28,  27,\n",
      "        -1,  -1,  30,  -1,  -1,  47,  40,  37,  36,  -1,  -1,  39,  -1,\n",
      "        -1,  44,  43,  -1,  -1,  46,  -1,  -1,  55,  52,  51,  -1,  -1,\n",
      "        54,  -1,  -1,  59,  58,  -1,  -1,  -1,  90,  77,  70,  67,  66,\n",
      "        -1,  -1,  69,  -1,  -1,  74,  73,  -1,  -1,  76,  -1,  -1,  83,\n",
      "        82,  81,  -1,  -1,  -1,  87,  86,  -1,  -1,  89,  -1,  -1, 106,\n",
      "        99,  96,  95,  -1,  -1,  98,  -1,  -1, 103, 102,  -1,  -1, 105,\n",
      "        -1,  -1, 114, 111, 110,  -1,  -1, 113,  -1,  -1, 118, 117,  -1,\n",
      "        -1, 120,  -1,  -1], dtype=int64), feature=array([ 1,  0,  1,  2,  1,  1, -2, -2,  0, -2, -2,  1,  2, -2, -2,  0, -2,\n",
      "       -2,  2,  1,  2, -2, -2, -2,  1,  1, -2, -2,  1, -2, -2,  0,  1,  2,\n",
      "        0, -2, -2,  2, -2, -2,  2,  2, -2, -2,  2, -2, -2,  1,  0,  2, -2,\n",
      "       -2,  0, -2, -2,  1,  0, -2, -2, -2,  0,  1,  2,  0,  0, -2, -2,  2,\n",
      "       -2, -2,  1,  0, -2, -2,  0, -2, -2,  2,  2,  2, -2, -2, -2,  0,  2,\n",
      "       -2, -2,  1, -2, -2,  1,  0,  2,  0, -2, -2,  0, -2, -2,  1,  1, -2,\n",
      "       -2,  0, -2, -2,  2,  1,  1, -2, -2,  1, -2, -2,  0,  1, -2, -2,  0,\n",
      "       -2, -2], dtype=int64), feature_uniq=array([0, 1, 2], dtype=int64), threshold=array([ 0.48772044,  0.31024949,  0.34650113,  0.4325754 ,  0.04981315,\n",
      "        0.03296066, -2.        , -2.        ,  0.19453967, -2.        ,\n",
      "       -2.        ,  0.3048199 ,  0.72979635, -2.        , -2.        ,\n",
      "        0.21775916, -2.        , -2.        ,  0.0885185 ,  0.44880481,\n",
      "        0.03876479, -2.        , -2.        , -2.        ,  0.47448742,\n",
      "        0.46510272, -2.        , -2.        ,  0.48084916, -2.        ,\n",
      "       -2.        ,  0.89187226,  0.17561367,  0.51823902,  0.36238694,\n",
      "       -2.        , -2.        ,  0.97146937, -2.        , -2.        ,\n",
      "        0.42705625,  0.40299471, -2.        , -2.        ,  0.44642481,\n",
      "       -2.        , -2.        ,  0.34458828,  0.8943913 ,  0.30927967,\n",
      "       -2.        , -2.        ,  0.9149552 , -2.        , -2.        ,\n",
      "        0.47230847,  0.9787882 , -2.        , -2.        , -2.        ,\n",
      "        0.50784841,  0.7769514 ,  0.39777312,  0.30258727,  0.22614995,\n",
      "       -2.        , -2.        ,  0.38605787, -2.        , -2.        ,\n",
      "        0.53491494,  0.23006961, -2.        , -2.        ,  0.46953796,\n",
      "       -2.        , -2.        ,  0.4891845 ,  0.48218688,  0.47438511,\n",
      "       -2.        , -2.        , -2.        ,  0.36096752,  0.95887625,\n",
      "       -2.        , -2.        ,  0.96195462, -2.        , -2.        ,\n",
      "        0.73251903,  0.87223247,  0.85544506,  0.52544814, -2.        ,\n",
      "       -2.        ,  0.80078825, -2.        , -2.        ,  0.52682698,\n",
      "        0.51952261, -2.        , -2.        ,  0.91668162, -2.        ,\n",
      "       -2.        ,  0.77595893,  0.74929968,  0.74183002, -2.        ,\n",
      "       -2.        ,  0.86601344, -2.        , -2.        ,  0.5243457 ,\n",
      "        0.8291221 , -2.        , -2.        ,  0.5756596 , -2.        ,\n",
      "       -2.        ]), max_depth=6, sample_weight=array([ 1.        ,  2.01257862,  3.        ,  1.44021739,  1.97849462,\n",
      "        6.64285714,  1.55555556,  2.8       ,  1.17721519,  1.54901961,\n",
      "        2.82142857,  2.02197802,  1.21333333,  2.02702703,  1.97368421,\n",
      "        5.6875    ,  2.        ,  2.        ,  3.27160494, 13.5       ,\n",
      "        1.2       ,  5.        ,  1.25      ,  6.        ,  1.08      ,\n",
      "        1.08695652,  1.0952381 , 11.5       , 12.5       ,  2.        ,\n",
      "        2.        ,  1.5       ,  1.20454545,  2.7672956 ,  1.89285714,\n",
      "       16.8       ,  1.06329114,  2.12      ,  1.05633803, 18.75      ,\n",
      "        1.5658363 ,  2.32231405,  1.09009009, 12.1       ,  1.75625   ,\n",
      "       26.66666667,  1.03896104,  5.88888889,  1.38461538, 21.66666667,\n",
      "        3.        ,  1.5       ,  1.0483871 ,  3.875     ,  1.34782609,\n",
      "        3.6       ,  1.04166667,  1.41176471,  3.42857143, 25.        ,\n",
      "        1.98757764,  1.92583732,  1.79399142,  2.4787234 ,  1.70909091,\n",
      "        1.14583333,  7.85714286,  2.41025641,  1.02631579, 39.        ,\n",
      "        1.67625899,  8.17647059,  2.42857143,  1.7       ,  1.13934426,\n",
      "        1.06086957, 17.42857143,  2.25945946,  2.20238095,  1.01204819,\n",
      "        1.01219512, 83.        , 84.        ,  1.83168317,  1.53030303,\n",
      "        1.08196721, 13.2       ,  2.88571429,  1.12903226,  8.75      ,\n",
      "        2.08010336,  2.02617801,  1.30821918,  1.21666667, 40.        ,\n",
      "        1.02564103,  5.61538462,  1.13043478,  8.66666667,  4.24444444,\n",
      "        5.625     ,  1.6       ,  2.66666667,  1.21621622,  2.84615385,\n",
      "        1.54166667,  1.9744898 ,  1.24840764, 19.625     ,  1.6       ,\n",
      "        2.66666667,  1.05369128,  2.12857143,  1.88607595,  5.02564103,\n",
      "       19.5       ,  2.        ,  2.        ,  1.05405405,  9.25      ,\n",
      "        1.12121212]), init_prediction=array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  9.45304082e-04, -2.84006355e-03,\n",
      "        0.00000000e+00,  7.93885092e-03,  1.20090941e-02,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.62785369e-02,  1.42014350e-02,  0.00000000e+00,\n",
      "       -4.62023880e-04,  2.83437601e-03,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.65316584e-04,  1.15510467e-03, -6.74601611e-04,\n",
      "        0.00000000e+00,  0.00000000e+00,  4.80616884e-02,  8.46953166e-03,\n",
      "        0.00000000e+00, -9.22732181e-04,  2.05692453e-03,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -9.17718625e-04,\n",
      "        3.49162022e-02,  0.00000000e+00,  5.78255030e-02, -1.46975858e-03,\n",
      "        0.00000000e+00,  0.00000000e+00,  9.38793986e-02,  4.49010060e-03,\n",
      "        0.00000000e+00,  1.00978456e-02,  1.56647105e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  1.33541636e-03,  4.55162182e-03,\n",
      "        0.00000000e+00,  1.19133577e-02,  5.29987510e-02,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.78509815e-02,  8.10916973e-03,  3.04423542e-03,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  3.76778667e-02,  1.95421789e-03,  0.00000000e+00,\n",
      "        4.80189163e-02, -7.56603719e-05,  0.00000000e+00,  0.00000000e+00,\n",
      "        3.26555256e-03,  1.06294111e-02,  0.00000000e+00,  1.54214978e-01,\n",
      "        4.95246354e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        1.07496576e-01,  2.23717918e-03,  3.74542455e-05,  0.00000000e+00,\n",
      "        0.00000000e+00,  9.14403012e-02,  4.34569122e-03,  0.00000000e+00,\n",
      "        5.71891416e-02,  1.00450341e-02,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  6.12578003e-03,  1.50571757e-01,\n",
      "        0.00000000e+00,  4.18951921e-02,  2.51966164e-03,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.39934470e-03,  3.15845570e-03,  0.00000000e+00,\n",
      "        2.79234266e-02,  4.13077987e-02,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  9.42550040e-03,  8.24846753e-03,  0.00000000e+00,\n",
      "        1.08834199e-01,  1.42434670e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "        2.64656030e-03,  2.87905140e-03,  0.00000000e+00,  5.06772302e-03,\n",
      "        6.80572306e-02]), node_count=121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2204973113.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  init_prediction[v] = tree.value[v] * n_v/n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import shap\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "from numba import njit\n",
    "from sklearn.datasets import make_regression\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class simple_tree:\n",
    "    \"\"\"\n",
    "    dataclass for a simple tree, in the scikit learn format that is necessary for computation\n",
    "    \n",
    "    Data:\n",
    "    children_left: left children_index\n",
    "    children_right: right children_index\n",
    "    feature: array of features splitted at each node\n",
    "    threshold: array of threshols for corresponding splitting features\n",
    "    max_depth: max_depth of the tree\n",
    "    n_node_samples: array of sample size for each node\n",
    "    value: array of values for each node, only leaf value is used, so only keep leaf value is fine\n",
    "    node_count: total number of leaves \n",
    "    \"\"\"\n",
    "    children_left: np.ndarray\n",
    "    children_right: np.ndarray\n",
    "    feature: np.ndarray\n",
    "    threshold: np.ndarray\n",
    "    max_depth: int\n",
    "    n_node_samples: np.ndarray\n",
    "    value: np.ndarray\n",
    "    node_count: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class tree_summary:\n",
    "    \"\"\"\n",
    "    dataclass for the calculation of cd-treeshap family\n",
    "    \n",
    "    Data:\n",
    "    - children_left: left children_index\n",
    "    - children_right: right children_index \n",
    "    - feature: array of features splitted at each node\n",
    "    - feature_uniq: array of uniq features\n",
    "    - threshold: array of threshols for corresponding splitting features\n",
    "    - max_depth: the max_depth of the tree\n",
    "    - sample_weight: list of sample size of parent/sample size of current node\n",
    "    - init_prediction: initial prediction from each leaf\n",
    "    - node_count: number of nodes\n",
    "    \"\"\"\n",
    "    children_left: np.ndarray\n",
    "    children_right: np.ndarray\n",
    "    feature: np.ndarray\n",
    "    feature_uniq: np.ndarray\n",
    "    threshold: np.ndarray\n",
    "    max_depth: int\n",
    "    sample_weight: np.ndarray\n",
    "    init_prediction: np.ndarray\n",
    "    node_count: int\n",
    "    \n",
    "\n",
    "    \n",
    "def summarize_tree(tree):\n",
    "    \"\"\"\n",
    "    Summarize the data needed for tree_summary. The tree object should have:\n",
    "    children_left: left children_index\n",
    "    children_right: right children_index\n",
    "    feature: array of features splitted at each node\n",
    "    threshold: array of threshols for corresponding splitting features\n",
    "    max_depth: max_depth of the tree\n",
    "    n_node_samples: array of sample size for each node\n",
    "    value: array of values for each node, only leaf value is used, so only keep leaf value is fine\n",
    "    node_count: total number of leaves \n",
    "    \"\"\"\n",
    "    sample_weight = np.ones_like(tree.threshold)\n",
    "    init_prediction = np.zeros_like(tree.threshold)\n",
    "    n = tree.n_node_samples[0]\n",
    "    \n",
    "    def traversal_summarize_tree(v):\n",
    "        v_l, v_r = tree.children_left[v], tree.children_right[v]\n",
    "        n_v, n_l, n_r = tree.n_node_samples[v], tree.n_node_samples[v_l], tree.n_node_samples[v_r]\n",
    "        \n",
    "        if v_l < 0:  #leaf\n",
    "            init_prediction[v] = tree.value[v] * n_v/n\n",
    "        else:\n",
    "            sample_weight[v_l], sample_weight[v_r] = n_v/n_l, n_v/n_r\n",
    "            traversal_summarize_tree(v_l)\n",
    "            traversal_summarize_tree(v_r)\n",
    "    \n",
    "    # travel from the root\n",
    "    traversal_summarize_tree(0)\n",
    "    \n",
    "    feature_uniq = np.unique(tree.feature[tree.feature >= 0])\n",
    "   \n",
    "    return tree_summary(tree.children_left, tree.children_right, tree.feature, feature_uniq, tree.threshold, tree.max_depth, sample_weight, init_prediction, tree.node_count)\n",
    "\n",
    "\n",
    "\n",
    "# Begin testing\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "max_depth = 6\n",
    "X = np.random.rand(n_samples, 3) \n",
    "y = X[:, 0] + 2 * X[:, 1] + 0.5 * X[:, 2] + np.random.randn(n_samples)  # Dependent variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Regressor\n",
    "tree_regressor = DecisionTreeRegressor(max_depth = max_depth)  # You can adjust max_depth as needed\n",
    "\n",
    "\n",
    "# Fit the Decision Tree to the training data\n",
    "tree_fit = tree_regressor.fit(X_train, y_train)\n",
    "start = time.time()\n",
    "summary_tree = summarize_tree(tree_fit.tree_)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(summary_tree)        \n",
    "\n",
    "if max_depth < 5:  \n",
    "    from sklearn.tree import plot_tree\n",
    "    plt.figure(figsize=(12, 6))\n",
    "   # plot_tree(tree_regressor, filled=True, feature_names=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "    plot_tree(tree_regressor, filled=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4fbb8",
   "metadata": {},
   "source": [
    "# Miscellaneous functions\n",
    "This module will introduce some functions used for calculating treeshap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95c2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0292769 +0.j         0.02262614+0.00823524j 0.01286629+0.0107961j\n",
      " 0.00487213+0.00843878j 0.00055254+0.00313362j 0.00055254-0.00313362j\n",
      " 0.00487213-0.00843878j 0.01286629-0.0107961j  0.02262614-0.00823524j]\n",
      "[0.0292769 +0.j         0.02262614+0.00823524j 0.01286629+0.0107961j\n",
      " 0.00487213+0.00843878j 0.00055254+0.00313362j 0.00055254-0.00313362j\n",
      " 0.00487213-0.00843878j 0.01286629-0.0107961j  0.02262614-0.00823524j]\n",
      "90.0\n",
      "119.0\n"
     ]
    }
   ],
   "source": [
    "d_test = 9\n",
    "\n",
    "def inv_binom_coef(d):\n",
    "    \"\"\"\n",
    "    calculate the inverse of binomial coefficients using an iterative method with symmetry.\n",
    "    \n",
    "    Parameters:\n",
    "    - d: dimension\n",
    "    \n",
    "    Example: \n",
    "    binom_coef(5)\n",
    "    ([ 1.,  5., 10., 10.,  5.,  1.])\n",
    "    \"\"\"\n",
    "    coef = np.zeros(d + 1)\n",
    "    coef[0] = 1\n",
    "    for i in range(1, d // 2 + 1):\n",
    "        coef[i] = coef[i - 1] * (d - i + 1) / i\n",
    "    for i in range(d // 2 + 1, d + 1):\n",
    "        coef[i] = coef[d - i]\n",
    "    return 1/coef\n",
    "\n",
    "#inv_binom_coef(d_test)\n",
    "\n",
    "def complex_v_invc_degree(d):\n",
    "    \"\"\"\n",
    "    Pre store v_invc: v(z)^-1 @ c / d at degree d where z are complex roots of unity\n",
    "    \n",
    "    Parameters:\n",
    "    -d: degree\n",
    "    -c: the coefficients matrix\n",
    "    \"\"\"\n",
    "    omega_inv = np.exp(-2 * np.pi * 1j * np.arange(d) / d)\n",
    "    v_omega_inv = np.vander(omega_inv, increasing=True)\n",
    "    v_inv_omega_theo = v_omega_inv / d\n",
    "    res = v_inv_omega_theo @ inv_binom_coef(d-1) / d\n",
    "    return res\n",
    "\n",
    "print(complex_v_invc_degree(d_test))\n",
    "\n",
    "\n",
    "def store_complex_v_invc(d):\n",
    "    \"\"\"\n",
    "    Pre store v_invc: v(z)^-1 @ c / d up to maximum tree depth where z are complex roots of unity\n",
    "    \n",
    "    Parameters:\n",
    "    -d: max treedepth\n",
    "    \"\"\"\n",
    "    res = np.zeros((d+1, d), dtype=complex)\n",
    "    \n",
    "    for i in range(1, d+1):\n",
    "        res[i, :i] = complex_v_invc_degree(i)\n",
    "    \n",
    "    return res \n",
    "\n",
    "# how to retrive the degree d_test pre_stored value\n",
    "print(store_complex_v_invc(20)[d_test, :d_test])\n",
    "\n",
    "def store_complex_root(d):\n",
    "    \"\"\"\n",
    "    Prestore the complex root of unity z\n",
    "    \n",
    "    Parameters:\n",
    "    -d: max treedepth\n",
    "    \"\"\" \n",
    "    res = np.zeros((d+1, d), dtype=complex)\n",
    "    \n",
    "    for i in range(1, d+1):\n",
    "        res[i, :i] = np.exp(2 * np.pi * 1j * np.arange(i) / i)\n",
    "    return res\n",
    "\n",
    "\n",
    "@njit\n",
    "def complex_dot_v2(p, v_invc, d):\n",
    "    \"\"\"\n",
    "    Return the dot product: C(z) * P(z) / d where z are the complex roots of unity, using the fact that P(w) and v_invc both:\n",
    "    except for the 0 index and possibly the last when d is odd, the rest are complex conjugate by head and tail, etc...\n",
    "    \n",
    "    Parameters \n",
    "    - p: a polynomial vector evaluated at complex root of unity\n",
    "    - v_invc: pre-calculated inverse coefficients\n",
    "    - d: the original degree before cut by half.\n",
    "    \"\"\"\n",
    "    len_p = len(p)\n",
    "    res = p[0] * v_invc[0]\n",
    "    if d % 2 == 0:\n",
    "        res += 2 * np.dot(p[1:(len_p-1)], v_invc[1:(len_p-1)]) + p[-1] * v_invc[-1]\n",
    "    else:\n",
    "        res += 2 * np.dot(p[1:len_p], v_invc[1:len_p])\n",
    "    return res.real\n",
    "\n",
    "# Example usage with numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "v1 = np.array([3, 6-1j, 5+1j, 6+1j])\n",
    "v2 = np.array([5, 4-1j, 6+1j, 4+1j])\n",
    "v3 = np.array([3, 6-1j, 5+1j])\n",
    "v4 = np.array([5, 4-1j, 6+1j])\n",
    "print(complex_dot_v2(v3, v4, len(v1)))\n",
    "\n",
    "v5 = np.array([3, 6-1j, 5+1j, 5-1j, 6+1j])\n",
    "v6 = np.array([5, 4-1j, 6+1j, 6-1j, 4+1j])\n",
    "v7 = np.array([3, 6-1j, 5+1j])\n",
    "v8 = np.array([5, 4-1j, 6+1j])\n",
    "print(complex_dot_v2(v7, v8, len(v5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed6175",
   "metadata": {},
   "source": [
    "# Iterative way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92cc9d",
   "metadata": {},
   "source": [
    "This weight matrix only calculates weight for leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00562eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample:[0.06965074 0.80381453 0.18167326]\n",
      "\n",
      " time: 0.3003041744232178\n",
      "\n",
      "[[3.         0.         1.97849462]\n",
      " [3.         0.         1.97849462]\n",
      " [4.64705882 0.         1.97849462]\n",
      " [0.         0.         1.97849462]\n",
      " [3.         0.         0.        ]\n",
      " [3.         0.         0.        ]\n",
      " [6.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [3.         0.         0.        ]\n",
      " [3.         0.         0.        ]\n",
      " [3.         0.         0.        ]\n",
      " [3.         0.         1.08      ]\n",
      " [3.         0.         1.08      ]\n",
      " [3.         0.         1.08      ]\n",
      " [3.         0.         1.08      ]\n",
      " [0.         0.         1.89285714]\n",
      " [0.         0.         1.89285714]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         2.53153153]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         3.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [3.77143142 0.         2.4787234 ]\n",
      " [0.         0.         2.4787234 ]\n",
      " [0.         0.         2.54395297]\n",
      " [0.         0.         0.        ]\n",
      " [4.67703349 0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [2.0430622  0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.92583732 4.4908511  2.25609756]\n",
      " [1.92583732 4.4908511  0.        ]\n",
      " [1.92583732 4.4908511  0.        ]\n",
      " [2.94711469 4.4908511  0.        ]\n",
      " [2.94711469 4.4908511  0.        ]\n",
      " [0.         5.07031576 0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         1.21666667]\n",
      " [0.         0.         1.21666667]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.        ]\n",
      " [0.         0.         1.24840764]\n",
      " [0.         0.         1.24840764]\n",
      " [0.         8.80198468 1.24840764]\n",
      " [0.         0.         1.24840764]\n",
      " [0.         7.84890354 0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         3.92445177 0.        ]\n",
      " [0.         3.92445177 0.        ]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0 s\n",
      "File: /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2730904637.py\n",
      "Function: traversal_weight at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           @njit\n",
      "     2                                           def traversal_weight(x, v, w, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth, met_feature):\n",
      "     3                                               \"\"\"\n",
      "     4                                               Calculate the weight in the treeSHAP. \n",
      "     5                                           \n",
      "     6                                               Parameters:\n",
      "     7                                               - x: one sample to be explained \n",
      "     8                                               - v: node index \n",
      "     9                                               - w: weight vector passed to the current node, for temporary usage\n",
      "    10                                               - children_left: left children_index\n",
      "    11                                               - children_right: right children_index \n",
      "    12                                               - feature: list of features splitted at each node\n",
      "    13                                               - threshold: list of threshold for corresponding features\n",
      "    14                                               - sample_weight: a list of sample size of parent/sample size of current node\n",
      "    15                                               - leaf_ind: leaf indices\n",
      "    16                                               - w_res, L * p matrix of weights, which records the modified weight for each leaf and each feature\n",
      "    17                                               - w_ind, L * p matrix of indicator matrix, which records the met of features for each leaf\n",
      "    18                                               - depth: current depth\n",
      "    19                                               - met_feature: record all the features met to now\n",
      "    20                                           \n",
      "    21                                               Update:\n",
      "    22                                               w_res\n",
      "    23                                               w_ind\n",
      "    24                                               met_feature\n",
      "    25                                               \"\"\"\n",
      "    26                                           \n",
      "    27                                               v_l, v_r = children_left[v], children_right[v]\n",
      "    28                                           \n",
      "    29                                               if v_l < 0:\n",
      "    30                                                   # match to the right location so the value for w_res corresponds to the same order of leaf_ind\n",
      "    31                                                   ind = (leaf_ind == v)\n",
      "    32                                                   #feature_tmp = met_feature[0:depth]\n",
      "    33                                                   for tmp_depth in range(depth):\n",
      "    34                                                       w_res[ind, met_feature[tmp_depth]] = w[tmp_depth]\n",
      "    35                                                       w_ind[ind, met_feature[tmp_depth]] = 1\n",
      "    36                                               else:\n",
      "    37                                                   split_feature = feature[v]\n",
      "    38                                                   split_threshold = threshold[v]\n",
      "    39                                           \n",
      "    40                                                   former_depth = np.arange(depth)[met_feature[0:depth] == split_feature]\n",
      "    41                                           \n",
      "    42                                                   if len(former_depth) != 0:\n",
      "    43                                                       former_depth = former_depth[-1]\n",
      "    44                                                   else:\n",
      "    45                                                       former_depth = depth  \n",
      "    46                                                       w[depth] = 1\n",
      "    47                                           \n",
      "    48                                                   met_feature = met_feature.copy()\n",
      "    49                                                   met_feature[depth] = split_feature\n",
      "    50                                           \n",
      "    51                                                   w_r = w.copy()\n",
      "    52                                           \n",
      "    53                                                   if x[split_feature] <= split_threshold:\n",
      "    54                                                       w[depth] = w[former_depth] * sample_weight[v_l]\n",
      "    55                                                       w_r[depth] = 0\n",
      "    56                                                   else:\n",
      "    57                                                       w_r[depth] = w_r[former_depth] * sample_weight[v_r]\n",
      "    58                                                       w[depth] = 0 \n",
      "    59                                           \n",
      "    60                                                   traversal_weight(x, v_l, w, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth+1, met_feature) \n",
      "    61                                                   traversal_weight(x, v_r, w_r, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth+1, met_feature)\n",
      "\n",
      "Total time: 0.544308 s\n",
      "File: /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2730904637.py\n",
      "Function: weight at line 64\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    64                                           def weight(x, summary_tree):\n",
      "    65     10000    1809000.0    180.9      0.3      p = len(x)\n",
      "    66     10000    1383000.0    138.3      0.3      d = summary_tree.max_depth\n",
      "    67                                               \n",
      "    68     10000    1374000.0    137.4      0.3      feature_uniq = summary_tree.feature_uniq\n",
      "    69                                               \n",
      "    70     10000   30210000.0   3021.0      5.6      leaf_ind = np.arange(summary_tree.node_count)[summary_tree.children_left==-1]\n",
      "    71                                           \n",
      "    72                                               # L * p matrix. Note that unused features are also labeled as 0 here for efficient storage.\n",
      "    73     10000    3424000.0    342.4      0.6      w_res = np.empty((len(leaf_ind), p))\n",
      "    74     10000   16354000.0   1635.4      3.0      w_res[:, feature_uniq] = 1\n",
      "    75                                               \n",
      "    76     10000    2617000.0    261.7      0.5      w = np.empty(d)\n",
      "    77                                               \n",
      "    78                                               # L * p matrix. [i, j] = 1 if the feature j is used by leaf i. [i, j] = 0 corresponds to 1 in the\n",
      "    79                                               # above matrix. This two sparse matrices together could make the weight matrix well-defined, and save the\n",
      "    80                                               # storage at the same time\n",
      "    81     10000    2920000.0    292.0      0.5      w_ind = np.empty((len(leaf_ind), p))\n",
      "    82     10000   13232000.0   1323.2      2.4      w_ind[:, feature_uniq] = 0\n",
      "    83                                               \n",
      "    84     10000   18575000.0   1857.5      3.4      met_feature = np.full(d, -1, dtype=int)\n",
      "    85                                               \n",
      "    86                                               # begin traversal from root\n",
      "    87     10000  448180000.0  44818.0     82.3      traversal_weight(x, 0, w, summary_tree.children_left, summary_tree.children_right, summary_tree.feature, summary_tree.threshold, summary_tree.sample_weight, leaf_ind, w_res, w_ind, 0, met_feature)\n",
      "    88     10000    4230000.0    423.0      0.8      return w_res, w_ind\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def traversal_weight(x, v, w, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth, met_feature):\n",
    "    \"\"\"\n",
    "    Calculate the weight in the treeSHAP. \n",
    "\n",
    "    Parameters:\n",
    "    - x: one sample to be explained \n",
    "    - v: node index \n",
    "    - w: weight vector passed to the current node, for temporary usage\n",
    "    - children_left: left children_index\n",
    "    - children_right: right children_index \n",
    "    - feature: list of features splitted at each node\n",
    "    - threshold: list of threshold for corresponding features\n",
    "    - sample_weight: a list of sample size of parent/sample size of current node\n",
    "    - leaf_ind: leaf indices\n",
    "    - w_res, L * p matrix of weights, which records the modified weight for each leaf and each feature\n",
    "    - w_ind, L * p matrix of indicator matrix, which records the met of features for each leaf\n",
    "    - depth: current depth\n",
    "    - met_feature: record all the features met to now\n",
    "\n",
    "    Update:\n",
    "    w_res\n",
    "    w_ind\n",
    "    met_feature\n",
    "    \"\"\"\n",
    "\n",
    "    v_l, v_r = children_left[v], children_right[v]\n",
    "\n",
    "    if v_l < 0:\n",
    "        # match to the right location so the value for w_res corresponds to the same order of leaf_ind\n",
    "        ind = (leaf_ind == v)\n",
    "        #feature_tmp = met_feature[0:depth]\n",
    "        for tmp_depth in range(depth):\n",
    "            w_res[ind, met_feature[tmp_depth]] = w[tmp_depth]\n",
    "            w_ind[ind, met_feature[tmp_depth]] = 1\n",
    "    else:\n",
    "        split_feature = feature[v]\n",
    "        split_threshold = threshold[v]\n",
    "\n",
    "        former_depth = np.arange(depth)[met_feature[0:depth] == split_feature]\n",
    "\n",
    "        if len(former_depth) != 0:\n",
    "            former_depth = former_depth[-1]\n",
    "        else:\n",
    "            former_depth = depth  \n",
    "            w[depth] = 1\n",
    "\n",
    "        met_feature = met_feature.copy()\n",
    "        met_feature[depth] = split_feature\n",
    "\n",
    "        w_r = w.copy()\n",
    "\n",
    "        if x[split_feature] <= split_threshold:\n",
    "            w[depth] = w[former_depth] * sample_weight[v_l]\n",
    "            w_r[depth] = 0\n",
    "        else:\n",
    "            w_r[depth] = w_r[former_depth] * sample_weight[v_r]\n",
    "            w[depth] = 0 \n",
    "\n",
    "        traversal_weight(x, v_l, w, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth+1, met_feature) \n",
    "        traversal_weight(x, v_r, w_r, children_left, children_right, feature, threshold, sample_weight, leaf_ind, w_res, w_ind, depth+1, met_feature)\n",
    "\n",
    "        \n",
    "def weight(x, summary_tree):\n",
    "    p = len(x)\n",
    "    d = summary_tree.max_depth\n",
    "    \n",
    "    feature_uniq = summary_tree.feature_uniq\n",
    "    \n",
    "    leaf_ind = np.arange(summary_tree.node_count)[summary_tree.children_left==-1]\n",
    "\n",
    "    # L * p matrix. Note that unused features are also labeled as 0 here for efficient storage.\n",
    "    w_res = np.empty((len(leaf_ind), p))\n",
    "    w_res[:, feature_uniq] = 1\n",
    "    \n",
    "    w = np.empty(d)\n",
    "    \n",
    "    # L * p matrix. [i, j] = 1 if the feature j is used by leaf i. [i, j] = 0 corresponds to 1 in the\n",
    "    # above matrix. This two sparse matrices together could make the weight matrix well-defined, and save the\n",
    "    # storage at the same time\n",
    "    w_ind = np.empty((len(leaf_ind), p))\n",
    "    w_ind[:, feature_uniq] = 0\n",
    "    \n",
    "    met_feature = np.full(d, -1, dtype=int)\n",
    "    \n",
    "    # begin traversal from root\n",
    "    traversal_weight(x, 0, w, summary_tree.children_left, summary_tree.children_right, summary_tree.feature, summary_tree.threshold, summary_tree.sample_weight, leaf_ind, w_res, w_ind, 0, met_feature)\n",
    "    return w_res, w_ind\n",
    "\n",
    "\n",
    "# test\n",
    "ind = 0\n",
    "x = X_test[ind]\n",
    "print(\"First sample:\" + str(x))\n",
    "start = time.time()\n",
    "a = weight(x, summary_tree)\n",
    "end = time.time()\n",
    "print(\"\\n time: \" + str(end - start) + \"\\n\")\n",
    "print(a[0])\n",
    "print(a[1])\n",
    "\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "def main():\n",
    "    # Assuming x and summary_tree are defined and set up correctly\n",
    "    lp = LineProfiler()\n",
    "    # Add the function to be profiled\n",
    "    lp.add_function(weight)\n",
    "    # Optionally, if you want to profile traversal_weight and it's called inside weight, add it too\n",
    "    lp.add_function(traversal_weight.__wrapped__)\n",
    "    \n",
    "    # Run the function multiple times\n",
    "    for _ in range(10000):  # Adjust the number of runs as needed\n",
    "        lp.runcall(weight, x, summary_tree)\n",
    "    \n",
    "    # Print out the profiling results\n",
    "    lp.print_stats()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     profiler = cProfile.Profile()\n",
    "#     profiler.run('main()')\n",
    "#     stats = pstats.Stats(profiler)\n",
    "#     stats.strip_dirs().sort_stats('cumulative').print_stats(50)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8db8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.80381453 0.36334652]\n"
     ]
    }
   ],
   "source": [
    "res = np.zeros_like(x)\n",
    "for i in range(x.shape[0]):\n",
    "    res[i] = x[i] * i\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf3e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [[0.06965074 0.80381453 0.18167326]\n",
      " [0.55805124 0.70494806 0.41863686]\n",
      " [0.30833843 0.29264205 0.56651827]\n",
      " [0.24499021 0.59294632 0.61242525]\n",
      " [0.54082552 0.75825584 0.15775159]\n",
      " [0.39113571 0.05202981 0.70457372]\n",
      " [0.70343042 0.33530341 0.10499074]\n",
      " [0.45956372 0.49306119 0.26039716]\n",
      " [0.49045881 0.22741463 0.25435648]\n",
      " [0.99337799 0.27436829 0.95224862]\n",
      " [0.72999056 0.17162968 0.52103661]\n",
      " [0.24804633 0.3315698  0.6317694 ]\n",
      " [0.10097628 0.42064702 0.73145025]\n",
      " [0.70340703 0.35307496 0.15442542]\n",
      " [0.96239507 0.11052229 0.63083181]\n",
      " [0.93239394 0.2153982  0.85833764]\n",
      " [0.12354668 0.46304438 0.91605091]\n",
      " [0.82076712 0.90884372 0.81552382]\n",
      " [0.04728931 0.28924748 0.73662394]\n",
      " [0.89996731 0.83447614 0.39913624]\n",
      " [0.33851449 0.57749619 0.85273616]\n",
      " [0.23542762 0.1528502  0.69291755]\n",
      " [0.10551829 0.28534037 0.3931698 ]\n",
      " [0.00924711 0.75249141 0.00623777]\n",
      " [0.10214669 0.54251256 0.60919391]\n",
      " [0.86219152 0.97291949 0.96083466]\n",
      " [0.1002582  0.22771359 0.72743971]\n",
      " [0.97021294 0.71748573 0.61076559]\n",
      " [0.94447416 0.728379   0.51673871]\n",
      " [0.09394051 0.5759465  0.9292962 ]\n",
      " [0.93712302 0.11802031 0.14090976]\n",
      " [0.6575199  0.19787216 0.12187918]\n",
      " [0.25526452 0.88462206 0.20645141]\n",
      " [0.45311077 0.80354529 0.47169307]\n",
      " [0.0397833  0.01383625 0.96749405]\n",
      " [0.29589198 0.30329192 0.35588915]\n",
      " [0.84450443 0.32126274 0.6520685 ]\n",
      " [0.58323993 0.88260323 0.21796021]\n",
      " [0.03129253 0.16510813 0.95860129]\n",
      " [0.44458729 0.33610227 0.88067812]\n",
      " [0.75677864 0.63606106 0.24002027]\n",
      " [0.85729755 0.51409537 0.81462434]\n",
      " [0.52076144 0.26720703 0.87739879]\n",
      " [0.31268984 0.88432423 0.95853234]\n",
      " [0.61668956 0.2424065  0.94216648]\n",
      " [0.96673774 0.53470817 0.02461453]\n",
      " [0.88099806 0.55593769 0.74160311]\n",
      " [0.37384057 0.78506638 0.41539496]\n",
      " [0.99033895 0.21689698 0.6630782 ]\n",
      " [0.07811175 0.29282458 0.66404537]\n",
      " [0.00196128 0.33197109 0.83820557]\n",
      " [0.97073144 0.00386035 0.17857997]\n",
      " [0.04252509 0.43254571 0.41248046]\n",
      " [0.11732048 0.10700414 0.58969472]\n",
      " [0.82410478 0.17771658 0.46492217]\n",
      " [0.01412696 0.53269415 0.88533555]\n",
      " [0.16329214 0.41999481 0.69350897]\n",
      " [0.34216208 0.73123268 0.70765973]\n",
      " [0.79503504 0.36922333 0.31261712]\n",
      " [0.9673378  0.86112301 0.61765698]\n",
      " [0.11027995 0.15876825 0.14327791]\n",
      " [0.77911099 0.80512749 0.76924712]\n",
      " [0.18961397 0.20831059 0.24146305]\n",
      " [0.02786372 0.94386458 0.96239674]\n",
      " [0.94076521 0.23464922 0.89808529]\n",
      " [0.91716755 0.65426271 0.31012969]\n",
      " [0.90160673 0.50850989 0.60819238]\n",
      " [0.5545084  0.18726653 0.3256022 ]\n",
      " [0.69699724 0.7786954  0.77740756]\n",
      " [0.04063404 0.67720305 0.14259143]\n",
      " [0.42244379 0.26550772 0.18440104]\n",
      " [0.70641058 0.02457702 0.63398692]\n",
      " [0.15641675 0.52204825 0.26266441]\n",
      " [0.16121821 0.09306714 0.19117376]\n",
      " [0.77345467 0.4062725  0.96309389]\n",
      " [0.93916091 0.50631222 0.99980858]\n",
      " [0.79936341 0.56406952 0.47114104]\n",
      " [0.77633368 0.27334971 0.38058287]\n",
      " [0.98510868 0.78339665 0.51898992]\n",
      " [0.04138781 0.39805402 0.86046989]\n",
      " [0.45369684 0.53657921 0.89667129]\n",
      " [0.9762734  0.94425982 0.13973015]\n",
      " [0.58630301 0.80244866 0.12012674]\n",
      " [0.57153993 0.33100876 0.16392685]\n",
      " [0.55921818 0.85464983 0.41823209]\n",
      " [0.76156295 0.09344546 0.14864159]\n",
      " [0.50866826 0.68176586 0.37538922]\n",
      " [0.49340696 0.65704368 0.46105022]\n",
      " [0.81733911 0.47314298 0.88228367]\n",
      " [0.6212299  0.81859614 0.74482495]\n",
      " [0.4833563  0.49339851 0.77746983]\n",
      " [0.14654046 0.95819137 0.00262486]\n",
      " [0.062636   0.2419017  0.43228148]\n",
      " [0.48641045 0.44836918 0.567846  ]\n",
      " [0.35766335 0.41754285 0.24767574]\n",
      " [0.10611795 0.31148372 0.68694574]\n",
      " [0.58188894 0.12636753 0.87682062]\n",
      " [0.80301515 0.57729671 0.04278381]\n",
      " [0.44006333 0.52071599 0.0035107 ]\n",
      " [0.95850972 0.8414333  0.28555722]]\n",
      "\n",
      "[[-1.16487043  3.11592778 -0.60037881]\n",
      " [ 0.03772056  1.7066539  -0.553281  ]\n",
      " [-0.88915477 -1.49491777  0.62660414]\n",
      " [-1.04193925  1.34533165  1.25140237]\n",
      " [ 1.12695406  2.27719041 -0.26460073]\n",
      " [ 0.16046804 -2.25225035  0.74107452]\n",
      " [ 0.71061948 -1.47107992 -0.45714778]\n",
      " [ 0.34114254  0.71804492 -0.0201169 ]\n",
      " [ 0.25243443 -1.03311053 -0.43693212]\n",
      " [ 1.98859578 -2.02919526  0.39004814]\n",
      " [ 0.26704115 -2.24463752  0.62688858]\n",
      " [-1.09928595 -1.98776318  0.35959336]\n",
      " [-1.17054034 -0.67915596  0.29079233]\n",
      " [ 0.26317778 -1.01613943 -0.46464657]\n",
      " [ 2.27915452 -2.23930382  0.30959796]\n",
      " [ 2.23325666 -2.27772453  0.39391652]\n",
      " [-1.26687497 -0.82630804  0.53427904]\n",
      " [ 1.39489136  5.14027496  1.30434426]\n",
      " [-1.33792901 -1.58762487  0.23430098]\n",
      " [ 1.67381678  2.02881955 -0.56309259]\n",
      " [-0.25632567  1.30408206  0.50703839]\n",
      " [-0.76274514 -1.73705319  0.74232993]\n",
      " [-1.19945027 -1.24182001 -0.54550111]\n",
      " [-1.79786702  1.47190131 -1.14548018]\n",
      " [-0.91554583  1.60635967  0.86398094]\n",
      " [ 1.47565177  5.43593655  0.92792227]\n",
      " [-1.14751244 -1.37982264  0.76986669]\n",
      " [ 2.73306797  1.57618858  0.2256479 ]\n",
      " [ 2.77365021  1.61677082  0.14448341]\n",
      " [-1.40053115  1.89980232  1.05552361]\n",
      " [ 2.87406202 -2.25458776 -0.27002559]\n",
      " [ 0.66960769 -1.38905633 -0.49815958]\n",
      " [-1.62245201  3.59282913 -0.61969858]\n",
      " [-0.17976884  2.43367266 -0.90322527]\n",
      " [-1.19478673 -1.75727994  0.26081377]\n",
      " [-1.10242826 -1.06034285 -0.41511513]\n",
      " [ 0.74982243 -1.54482486  0.39496319]\n",
      " [ 1.36474148  4.48776064 -0.57950663]\n",
      " [-1.14907637 -1.97391355  0.43173702]\n",
      " [ 0.68733975 -1.63954781  0.55216882]\n",
      " [ 0.6163818   1.21924796 -0.64453631]\n",
      " [ 0.77616381 -0.07089929  0.48582893]\n",
      " [ 1.56002673 -2.68362415  0.72355818]\n",
      " [-1.42899746  3.18499725  0.94768322]\n",
      " [ 0.90632142 -2.0058337   0.69947305]\n",
      " [ 3.52911693  1.28374992 -0.2779624 ]\n",
      " [ 4.38618788  4.13703649  0.23908154]\n",
      " [-0.19853482  2.86282309 -1.31360973]\n",
      " [ 1.7537012  -1.67869172  0.27443917]\n",
      " [-1.14751244 -1.37982264  0.76986669]\n",
      " [-1.40106255 -1.9228754   0.28367333]\n",
      " [ 2.91973228 -2.29513428 -0.27514933]\n",
      " [-0.62544108 -0.57216352 -0.36129938]\n",
      " [-0.86189068 -1.70156413  0.80598641]\n",
      " [ 0.45294383 -1.11615638  0.26317332]\n",
      " [-2.36984236 -0.45023681  0.3284068 ]\n",
      " [-1.12510832 -0.86575372  0.43195806]\n",
      " [-0.25093352  1.3094742   0.4962541 ]\n",
      " [ 0.26169342 -1.0176238  -0.46167784]\n",
      " [ 1.33652179  1.86302305 -0.0600011 ]\n",
      " [-0.90919016 -1.66173712 -0.41584412]\n",
      " [ 0.34343676  2.87219272 -0.07608574]\n",
      " [-1.25769628 -1.30855345 -0.42052166]\n",
      " [-2.68722729  2.48457302 -0.9123217 ]\n",
      " [ 2.23325666 -2.27772453  0.39391652]\n",
      " [ 3.32539709  1.55996727 -0.35045992]\n",
      " [ 2.34374641  0.09378972  0.12009153]\n",
      " [ 0.58109615 -1.34702711 -0.45167726]\n",
      " [ 1.33415165  4.52621152  1.97914742]\n",
      " [-1.56291854  1.04775446 -0.95628181]\n",
      " [ 0.42419554 -1.11899108 -0.52281268]\n",
      " [ 0.27884874 -2.2682527   0.63869617]\n",
      " [-1.51555177  0.07972104 -0.03561516]\n",
      " [-0.90919016 -1.66173712 -0.41584412]\n",
      " [ 0.57440787 -1.59666702  0.62221992]\n",
      " [ 2.51298962 -0.09939938  0.14403741]\n",
      " [ 0.15287163  1.20678658 -0.16856476]\n",
      " [ 0.66246503 -1.38771155 -0.4923617 ]\n",
      " [ 1.09819856  2.09887702 -0.05753184]\n",
      " [-1.24582818 -0.7544438   0.44136801]\n",
      " [-0.34840082  1.14685793  0.75633767]\n",
      " [ 2.51175151  3.26647468 -0.5052307 ]\n",
      " [ 0.79065389  3.06364525 -0.71475541]\n",
      " [ 0.6292506  -1.43039548 -0.41646334]\n",
      " [ 0.40324862  3.13828171 -0.40198659]\n",
      " [ 0.34210483 -2.41824897 -0.47257879]\n",
      " [ 4.26704088  3.66555383 -0.30756874]\n",
      " [-1.83087014  0.37090202 -0.30743002]\n",
      " [-0.96194337  0.01343981  0.54846433]\n",
      " [ 0.34343676  2.87219272 -0.07608574]\n",
      " [-0.19508827 -0.05425676  0.09294188]\n",
      " [-1.63537102  3.72923343 -0.74318387]\n",
      " [-2.08119439 -1.28092628  0.37534928]\n",
      " [-0.12009281 -0.70295248  0.42300606]\n",
      " [-0.28477374 -0.45997257 -0.47286191]\n",
      " [-1.31832715 -2.07216977  0.3502323 ]\n",
      " [ 0.62126831 -2.86602814  0.89405204]\n",
      " [ 0.36260493  1.09667164 -0.26818312]\n",
      " [ 0.52071832  0.62366169 -0.10530947]\n",
      " [ 1.93615559  1.84537595 -0.6419878 ]]\n",
      "T2 time: 0.5379548072814941\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.143173 s\n",
      "File: /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3893516857.py\n",
      "Function: T2 at line 42\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    42                                           def T2(x, summary_tree, ncore=-1):\n",
      "    43                                               \"\"\"\n",
      "    44                                               Calculate the second order treeshap value\n",
      "    45                                               \n",
      "    46                                               Parameters:\n",
      "    47                                               -x: sample to be explained\n",
      "    48                                               -summary_tree: summary tree\n",
      "    49                                               -ncore: number of cores to use, with default value -1 to utilize all the cores\n",
      "    50                                               \n",
      "    51                                               Return:\n",
      "    52                                               treeshap value for the sample\n",
      "    53                                               \"\"\"\n",
      "    54         1      10000.0  10000.0      0.0      max_core = os.cpu_count()\n",
      "    55         1          0.0      0.0      0.0      if ncore == -1:\n",
      "    56         1       1000.0   1000.0      0.0           ncore = os.cpu_count()\n",
      "    57         1       1000.0   1000.0      0.0      ncore = min(max_core, ncore)\n",
      "    58                                               \n",
      "    59         1      25000.0  25000.0      0.0      init_prediction = summary_tree.init_prediction[summary_tree.init_prediction!=0]\n",
      "    60         1          0.0      0.0      0.0      d = summary_tree.max_depth\n",
      "    61                                               \n",
      "    62                                               # store v_inc * c / d evaluated at complex roots \n",
      "    63         1     493000.0 493000.0      0.3      store_v_invc = store_complex_v_invc(d * 2)\n",
      "    64         1      41000.0  41000.0      0.0      store_z = store_complex_root(d * 2)\n",
      "    65                                               \n",
      "    66         1       9000.0   9000.0      0.0      shap_value = np.zeros_like(x)\n",
      "    67                                               \n",
      "    68       101      45000.0    445.5      0.0      for i in range(x.shape[0]):\n",
      "    69       100      96000.0    960.0      0.1              xi = x[i]\n",
      "    70       100    6415000.0  64150.0      4.5              w = weight(xi, summary_tree)\n",
      "    71       100      51000.0    510.0      0.0              w_matrix = w[0]\n",
      "    72       100      26000.0    260.0      0.0              w_ind = w[1]\n",
      "    73                                           \n",
      "    74       100  135959000.0    1e+06     95.0              T2_sample(i, w_matrix, w_ind, init_prediction, store_v_invc, store_z, shap_value, summary_tree.feature_uniq)\n",
      "    75                                           \n",
      "    76         1       1000.0   1000.0      0.0      return shap_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import concurrent.futures\n",
    "import os\n",
    "from numba import complex128, int32\n",
    "\n",
    "@njit\n",
    "def T2_sample(i, w_matrix, w_ind, init_prediction, store_v_invc, store_z, shap_value, feature_uniq):\n",
    "    ## Calculate T2 for each sample\n",
    "    L, p = w_matrix.shape\n",
    "    \n",
    "    for l1 in range(L):\n",
    "        for l2 in range(l1, L):\n",
    "            init_prediction_product = init_prediction[l1] * init_prediction[l2]\n",
    "            \n",
    "            union_f12 = feature_uniq[(w_ind[l1, feature_uniq] + w_ind[l2, feature_uniq]) >= 1]\n",
    "            \n",
    "            n12 = len(union_f12)\n",
    "            # begin to use the property of complex conjugate\n",
    "            n12_c = n12 // 2 + 1\n",
    "\n",
    "            v_invc = store_v_invc[n12, :n12_c]\n",
    "            z = store_z[n12, :n12_c]\n",
    "\n",
    "            p_z = np.zeros((n12_c), dtype=complex128)\n",
    "            tmp_p_z = np.zeros((n12_c), dtype=complex128)\n",
    "\n",
    "            for k in range(n12_c):\n",
    "                p_z[k] = np.prod(z[k] + w_matrix[l1, union_f12] * w_matrix[l2, union_f12])\n",
    "\n",
    "            # update only when feature j belongs to the union of f1 and f2\n",
    "            for j in union_f12:\n",
    "                    # remove the operation for j by dividing\n",
    "                for k in range(n12_c):\n",
    "                    tmp_p_z[k] = p_z[k] / (z[k] + w_matrix[l1, j] * w_matrix[l2, j])\n",
    "                    \n",
    "                if l1 != l2:\n",
    "                    shap_value[i, j] += 2 * (w_matrix[l1, j] * w_matrix[l2, j] - 1) * init_prediction_product * complex_dot_v2(tmp_p_z, v_invc, n12)\n",
    "                else:\n",
    "                    shap_value[i, j] += (w_matrix[l1, j] * w_matrix[l2, j] - 1) * init_prediction_product * complex_dot_v2(tmp_p_z, v_invc, n12)\n",
    "\n",
    "                    \n",
    "def T2(x, summary_tree, ncore=-1):\n",
    "    \"\"\"\n",
    "    Calculate the second order treeshap value\n",
    "    \n",
    "    Parameters:\n",
    "    -x: sample to be explained\n",
    "    -summary_tree: summary tree\n",
    "    -ncore: number of cores to use, with default value -1 to utilize all the cores\n",
    "    \n",
    "    Return:\n",
    "    treeshap value for the sample\n",
    "    \"\"\"\n",
    "    max_core = os.cpu_count()\n",
    "    if ncore == -1:\n",
    "         ncore = os.cpu_count()\n",
    "    ncore = min(max_core, ncore)\n",
    "    \n",
    "    init_prediction = summary_tree.init_prediction[summary_tree.init_prediction!=0]\n",
    "    d = summary_tree.max_depth\n",
    "    \n",
    "    # store v_inc * c / d evaluated at complex roots \n",
    "    store_v_invc = store_complex_v_invc(d * 2)\n",
    "    store_z = store_complex_root(d * 2)\n",
    "    \n",
    "    shap_value = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "            xi = x[i]\n",
    "            w = weight(xi, summary_tree)\n",
    "            w_matrix = w[0]\n",
    "            w_ind = w[1]\n",
    "\n",
    "            T2_sample(i, w_matrix, w_ind, init_prediction, store_v_invc, store_z, shap_value, summary_tree.feature_uniq)\n",
    "\n",
    "    return shap_value\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# Re-run the first trunk \n",
    "ind = range(100)\n",
    "x = np.array(X_test[ind])\n",
    "y = np.array(y_test[ind])\n",
    "# x.shape = 1, 3\n",
    "\n",
    "print(\"Sample: \" + str(x) + \"\\n\")\n",
    "\n",
    "T2_x = T2(x, summary_tree)\n",
    "print(T2_x)\n",
    "print(\"T2 time: \" + str(time.time() - start))\n",
    "\n",
    "#print(\"Sample: \" + str(x) + \"\\n\")\n",
    "\n",
    "# def main():\n",
    "#     for _ in range(10):\n",
    "#         T2_x = T2(x, summary_tree)\n",
    "#     #print(T2_x)\n",
    "#     pass\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     cProfile.run('main()', 'main.stats')\n",
    "#     main()\n",
    "    \n",
    "\n",
    "# # Load the stats file\n",
    "# p = pstats.Stats('main.stats')\n",
    "\n",
    "# #Sort the statistics by cumulative time and print the first few lines\n",
    "# p.sort_stats('cumulative').print_stats(100)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Assuming x and summary_tree are defined and set up correctly\n",
    "    lp = LineProfiler()\n",
    "    # Add the function to be profiled\n",
    "    lp.add_function(T2)\n",
    "    \n",
    "    lp.runcall(T2, x, summary_tree)\n",
    "    \n",
    "    # Print out the profiling results\n",
    "    lp.print_stats()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f17ac8",
   "metadata": {},
   "source": [
    "# square_treeshap, loss_treeshap, and cd_treeshap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c6480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square_treeshp: [[-1.16487043  3.11592778 -0.60037881]\n",
      " [ 0.03772056  1.7066539  -0.553281  ]\n",
      " [-0.88915477 -1.49491777  0.62660414]]\n",
      "\n",
      "My loss treeshap: [[-0.65075585  1.82708098 -0.35230783]\n",
      " [ 0.01748439  1.11573346 -0.35052859]\n",
      " [-0.40682569 -0.70104209  0.26213723]]\n",
      "Loss sum + intercept: [1.81749189 1.97890908 0.15108101]\n",
      "Real loss: [1.81749189 1.97890908 0.15108101]\n",
      "\n",
      "cd_treeshap: [0.11493914 0.29050581 0.04686456]\n",
      "The Model R^2 is: 0.45230950592279173\n",
      "\n",
      "My sum of cd_treeshap and the real R^2 is equal?:  True!!!!!\n"
     ]
    }
   ],
   "source": [
    "def loss_treeshap(x, y, summary_tree, explainer, learning_rate=1):\n",
    "    \"\"\"\n",
    "    Explain l2 loss for every sample\n",
    "    \n",
    "    Parameters:\n",
    "    -x: samples\n",
    "    -y: y corresponding to x\n",
    "    -summary_tree: summary tree\n",
    "    -learning_rate: learning_rate if it's a tree from ensemble methods, learning_rate for decision treeshould be 1\n",
    "    \n",
    "    Return:\n",
    "    Global treeshap value\n",
    "    \"\"\"\n",
    "    square_treeshap_x = T2(x, summary_tree) * learning_rate ** 2 \n",
    "    # direct call from shap\n",
    "    T0_x = explainer.shap_values(x) * learning_rate \n",
    "    res = square_treeshap_x - 2 * (y * T0_x.T).T\n",
    "    return res \n",
    "\n",
    "def cd_treeshap(x, y, summary_tree, explainer):\n",
    "    \"\"\"\n",
    "    Calculate the cd-treeshap value\n",
    "    \n",
    "    Parameters:\n",
    "    -x: samples\n",
    "    -y: y corresponding to x\n",
    "    -summary_tree: summary tree\n",
    "    \n",
    "    Return:\n",
    "    Global treeshap value\n",
    "    \"\"\"\n",
    "    sst = np.sum((y-np.mean(y)) ** 2)\n",
    "    T0_x = explainer.shap_values(x)\n",
    "    square_shap = T2(x, summary_tree)\n",
    "    res_indv = 1/sst * (-square_shap + 2 * (y * T0_x.T).T)\n",
    "    res = np.sum(res_indv, axis = 0)\n",
    "    return res\n",
    "\n",
    "\n",
    "# Re-run the first trunk \n",
    "ind = range(3)\n",
    "x = np.array(X_test[ind])\n",
    "y = np.array(y_test[ind])\n",
    "ypred = tree_regressor.predict(x)\n",
    "# x.shape = 1, 3\n",
    "\n",
    "explainer = shap.TreeExplainer(tree_regressor)\n",
    "square_treehshap_x = T2(x, summary_tree)\n",
    "print(\"square_treeshp: \" + str(square_treehshap_x) + \"\\n\")\n",
    "\n",
    "\n",
    "loss_treeshap_xy = loss_treeshap(x, y, summary_tree, explainer)\n",
    "print(\"My loss treeshap: \" + str(loss_treeshap_xy))\n",
    "print(\"Loss sum + intercept: \" + str(np.sum(loss_treeshap_xy, axis=1) + (y - np.mean(y_train)) ** 2))\n",
    "print(\"Real loss: \" + str((y - ypred) ** 2) + \"\\n\")\n",
    "\n",
    "\n",
    "x = np.array(X_train)\n",
    "y = np.array(y_train)\n",
    "# x.shape = 1, 3\n",
    "cd_treeshap_res = cd_treeshap(x, y, summary_tree, explainer)\n",
    "print(\"cd_treeshap: \" + str(cd_treeshap_res))\n",
    "\n",
    "# Let's check the real R^2\n",
    "ypred = tree_regressor.predict(x)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "sse = np.sum((y - ypred) ** 2)\n",
    "rsq = 1 - sse/sst\n",
    "print(\"The Model R^2 is: \" + str(rsq) + \"\\n\")\n",
    "print(\"My sum of cd_treeshap and the real R^2 is equal?:  \" + str(round(np.sum(cd_treeshap_res), 5)==round(rsq, 5)) + \"!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5319ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(tree_regressor, sklearn.tree.DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b655f",
   "metadata": {},
   "source": [
    "**This test is tiny, let's get a larger one, which could not be easily computed by moden computer by brute force**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1af1904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2204973113.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  init_prediction[v] = tree.value[v] * n_v/n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.992902755737305\n",
      "Wed Feb  5 10:34:05 2025    main.stats\n",
      "\n",
      "         11765 function calls (11761 primitive calls) in 11.734 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 193 to 100 due to restriction <100>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000   11.729   11.729 {built-in method builtins.exec}\n",
      "      2/1    0.191    0.096   11.729   11.729 <string>:1(<module>)\n",
      "       10    9.144    0.914    9.291    0.929 {built-in method time.sleep}\n",
      "        1    0.001    0.001    7.230    7.230 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2051348148.py:26(main)\n",
      "        1    0.004    0.004    7.230    7.230 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/1601285480.py:20(cd_treeshap)\n",
      "        3    0.000    0.000    4.293    1.431 /opt/anaconda3/lib/python3.12/asyncio/base_events.py:1910(_run_once)\n",
      "        3    0.369    0.123    4.281    1.427 /opt/anaconda3/lib/python3.12/selectors.py:558(select)\n",
      "        1    0.965    0.965    0.984    0.984 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3893516857.py:42(T2)\n",
      "        3    0.841    0.280    0.854    0.285 {method 'control' of 'select.kqueue' objects}\n",
      "     1000    0.185    0.000    0.190    0.000 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2730904637.py:64(weight)\n",
      "        1    0.000    0.000    0.014    0.014 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:369(shap_values)\n",
      "        4    0.000    0.000    0.013    0.003 /opt/anaconda3/lib/python3.12/asyncio/events.py:86(_run)\n",
      "        4    0.000    0.000    0.013    0.003 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py:200(_handle_events)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:607(_handle_events)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:648(_handle_recv)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:580(_run_callback)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:156(_handle_event)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:275(<lambda>)\n",
      "        1    0.000    0.000    0.013    0.013 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:277(_really_send)\n",
      "        8    0.012    0.002    0.013    0.002 /opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py:621(send)\n",
      "        1    0.011    0.011    0.011    0.011 {built-in method shap._cext.dense_tree_shap}\n",
      "     1000    0.002    0.000    0.002    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/numeric.py:274(full)\n",
      "     1028    0.002    0.000    0.002    0.000 {built-in method numpy.arange}\n",
      "     4014    0.001    0.000    0.001    0.000 {built-in method numpy.empty}\n",
      "        1    0.000    0.000    0.001    0.001 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:1352(predict)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method shap._cext.dense_tree_predict}\n",
      "       19    0.001    0.000    0.001    0.000 {built-in method numpy.zeros}\n",
      "        1    0.001    0.001    0.001    0.001 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/numeric.py:67(zeros_like)\n",
      "        1    0.000    0.000    0.001    0.001 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:312(_validate_inputs)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "     3049    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:660(assert_additivity)\n",
      "        3    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:71(_wrapreduction)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2177(sum)\n",
      "        1    0.000    0.000    0.000    0.000 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3544483469.py:41(store_complex_v_invc)\n",
      "       14    0.000    0.000    0.000    0.000 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3544483469.py:24(complex_v_invc_degree)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/tornado/ioloop.py:742(_run_callback)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:585(_flush)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:47(_sum)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:754(send)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py:698(send_multipart)\n",
      "     1001    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/multiarray.py:1080(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:690(serialize)\n",
      "       14    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/lib/twodim_base.py:534(vander)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/attrsettr.py:42(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:270(send_multipart)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:258(schedule)\n",
      "        4    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:92(json_packer)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/attrsettr.py:65(_get_attr_opt)\n",
      "        4    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/json/__init__.py:183(dumps)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:687(_rebuild_io_state)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:662(check_sum)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:117(_run_event_pipe_gc)\n",
      "        1    0.000    0.000    0.000    0.000 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3544483469.py:58(store_complex_root)\n",
      "        4    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/json/encoder.py:183(encode)\n",
      "       14    0.000    0.000    0.000    0.000 /var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/3544483469.py:3(inv_binom_coef)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:710(_update_handler)\n",
      "        4    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/json/encoder.py:205(iterencode)\n",
      "        7    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/enum.py:1551(__or__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:649(msg)\n",
      "    88/86    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/asyncio/tasks.py:653(sleep)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:675(sign)\n",
      "       13    0.000    0.000    0.000    0.000 {method 'accumulate' of 'numpy.ufunc' objects}\n",
      "       30    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/enum.py:1544(_get_value)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:645(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3385(mean)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/traitlets/traitlets.py:708(__set__)\n",
      "       15    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/traitlets/traitlets.py:676(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/jsonutil.py:107(json_default)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py:777(recv_multipart)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/enum.py:726(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/traitlets/traitlets.py:689(set)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:126(_event_pipe_gc)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:101(_mean)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/asyncio/futures.py:311(_set_result_unless_cancelled)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2692(max)\n",
      "        3    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/enum.py:1562(__and__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/traitlets/traitlets.py:718(_validate)\n",
      "        4    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/threading.py:1220(is_alive)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method posix.getppid}\n",
      "        8    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/asyncio/base_events.py:734(time)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method posix.cpu_count}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/asyncio/base_events.py:743(call_later)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/jupyter_client/session.py:272(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/ipykernel/iostream.py:678(_flush_buffer)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/hmac.py:122(copy)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/typing.py:1212(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'set_result' of '_asyncio.Future' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:519(_get_shap_output)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/asyncio/base_events.py:767(call_at)\n",
      "        3    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/typing.py:392(inner)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/anaconda3/lib/python3.12/typing.py:1483(__subclasscheck__)\n",
      "\n",
      "\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.00804584e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.49131907e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.48417036e-04 -1.06700517e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.74921208e-04  0.00000000e+00\n",
      "  0.00000000e+00  4.60871743e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  9.52262343e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.40089338e-06  1.02320625e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.48544638e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -5.37328568e-06  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  4.51547488e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00371977e-03\n",
      "  4.56661913e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.05658214e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.27027002e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.04694725e-03\n",
      "  0.00000000e+00  1.18311745e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.34544746e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.55459127e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.29050458e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.40566710e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.88425581e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.18469334e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.07836752e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.88107673e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -3.31300880e-06  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.15631474e-07\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.90221069e-01  0.00000000e+00  6.75912341e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.18693447e-06\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.15027166e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  8.34286648e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.18941975e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.13780080e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  9.92433754e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.77049104e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -7.24564380e-05  0.00000000e+00  0.00000000e+00 -2.46055312e-05\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.74932020e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.50567357e-06\n",
      "  2.23782708e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.33758540e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.06970979e-04  1.26299205e-06  0.00000000e+00\n",
      "  0.00000000e+00  1.02486894e-08  0.00000000e+00  0.00000000e+00\n",
      "  1.35585238e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  8.10664802e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.61611953e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.50230741e-04  0.00000000e+00  1.27358202e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  5.03857718e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.43914408e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.27111565e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.47494590e-05  9.91687810e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.48752492e-04  0.00000000e+00\n",
      "  1.22398070e-03  1.43095141e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  7.03401032e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.52826208e-03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.17027900e-04  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.10774091e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.22200511e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.70237310e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  8.86395569e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.88189708e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -4.72145451e-07 -5.30529850e-06  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.25848085e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.49491674e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.37199193e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.83641429e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  5.08742714e-04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.65319982e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.89428478e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.40931823e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  6.64134270e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.59193057e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "My R^2 sum is: 0.9290441873224276\n",
      "Model R^2 is: 0.9290441873224277\n",
      "\n",
      "My sum of cd_treeshap and the model R^2 is equal?:  True!!!!!\n"
     ]
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "np.random.seed(0)\n",
    "x, y, coefficients = make_regression(n_samples=1000, n_features=1000, n_informative=5, coef=True, random_state=0)\n",
    "\n",
    "max_depth = 7\n",
    "tree_regressor = DecisionTreeRegressor(max_depth=max_depth)\n",
    "tree_fit = tree_regressor.fit(x, y)\n",
    "\n",
    "summary_tree = summarize_tree(tree_fit.tree_)\n",
    "\n",
    "start = time.time()\n",
    "explainer = shap.TreeExplainer(tree_regressor)\n",
    "cd_treeshap_res = cd_treeshap(x, y, summary_tree, explainer)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \" + str(end - start))\n",
    "#print(\"cd_treeshap: \" + str(cd_treeshap_res))\n",
    "\n",
    "# Let's check the real R^2\n",
    "ypred = tree_regressor.predict(x)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "sse = np.sum((y - ypred) ** 2)\n",
    "rsq = 1 - sse/sst\n",
    "\n",
    "def main():\n",
    "    cd_treeshap(x, y, summary_tree, explainer)\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cProfile.run('main()', 'main.stats')\n",
    "    main()\n",
    "    \n",
    "\n",
    "# Load the stats file\n",
    "p = pstats.Stats('main.stats')\n",
    "\n",
    "# Sort the statistics by cumulative time and print the first few lines\n",
    "p.sort_stats('cumulative').print_stats(100)\n",
    "\n",
    "print(cd_treeshap_res)\n",
    "print(\"My R^2 sum is: \" + str(np.sum(cd_treeshap_res)))\n",
    "print(\"Model R^2 is: \" + str(rsq) + \"\\n\")\n",
    "print(\"My sum of cd_treeshap and the model R^2 is equal?:  \" + str(round(np.sum(cd_treeshap_res), 5)==round(rsq, 5)) + \"!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a1ac7",
   "metadata": {},
   "source": [
    "# GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179fb914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.66874098]\n",
      " [0.66874098 1.        ]]\n",
      "[[1.        0.6817336]\n",
      " [0.6817336 1.       ]]\n",
      "[[1.         0.70735073]\n",
      " [0.70735073 1.        ]]\n",
      "[[1.         0.74599082]\n",
      " [0.74599082 1.        ]]\n",
      "[2.28334546 2.28334546 2.28334546 ... 1.99739531 1.99739531 1.99739531]\n",
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
      "<shap.explainers._tree.TreeExplainer object at 0x3108e9730>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2204973113.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  init_prediction[v] = tree.value[v] * n_v/n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X, y = california_housing.data, california_housing.target\n",
    "\n",
    "\n",
    "# Train GBM model\n",
    "gbm = GradientBoostingRegressor(n_estimators=10, max_depth=2)\n",
    "gbm.fit(X, y)\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=10)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Extract trees from the GBM model\n",
    "trees = gbm.estimators_\n",
    "\n",
    "# Summarize each tree\n",
    "summary_trees = []\n",
    "for tree in trees:\n",
    "    summary_trees.append(summarize_tree(tree[0].tree_))\n",
    "\n",
    "\n",
    "# Now you have summary_trees, which contains the summarized information of each tree\n",
    "summary_trees\n",
    "\n",
    "first_pred = trees[0, 0].predict(X)\n",
    "second_pred = first_pred + trees[1, 0].predict(X)\n",
    "third_pred = second_pred + trees[2, 0].predict(X)\n",
    "\n",
    "final_pred = gbm.predict(X)\n",
    "\n",
    "#print(first_pred)\n",
    "#print(second_pred)\n",
    "#print(y)\n",
    "\n",
    "print(np.corrcoef(y, first_pred))\n",
    "print(np.corrcoef(y, second_pred))\n",
    "print(np.corrcoef(y, third_pred))\n",
    "print(np.corrcoef(y, final_pred))\n",
    "\n",
    "# get preidction for each stage  \n",
    "staged_predictions = list(gbm.staged_predict(X))[0]\n",
    "print(staged_predictions)\n",
    "print(y)\n",
    "\n",
    "len(trees)\n",
    "\n",
    "explainer = shap.TreeExplainer(trees[1, 0])\n",
    "print(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d7a1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(gbm, sklearn.ensemble.GradientBoostingRegressor)\n",
    "gbm.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a775b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/qq25rs5s3wn9yqrktd830cvc0000gp/T/ipykernel_12542/2204973113.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  init_prediction[v] = tree.value[v] * n_v/n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.408384084701538\n",
      "[0.    0.    0.    0.    0.037 0.    0.076 0.    0.    0.    0.    0.\n",
      " 0.103 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.038 0.049 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.313 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.02\n",
      " 0.    0.    0.    0.    0.    0.306 0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.029 0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "treeshap_rsq: 0.974656574862846\n",
      "Model rsq: 0.9746565748628457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y, coefficients = make_regression(n_samples=2000, n_features=100, n_informative=10, coef=True, random_state=0)\n",
    "\n",
    "tree_regressor = GradientBoostingRegressor(max_depth=3)\n",
    "#tree_regressor = RandomForestRegressor(n_estimators=100, max_depth=max_depth, learning_rate = 0.01)\n",
    "\n",
    "tree_fit = tree_regressor.fit(x, y)\n",
    "ypred = tree_regressor.predict(x)\n",
    "ensemble_tree = tree_regressor.estimators_\n",
    "staged_predict = list(tree_fit.staged_predict(x))\n",
    "#learning rate\n",
    "alpha = tree_regressor.learning_rate\n",
    "\n",
    "num_tree = len(tree_fit)\n",
    "\n",
    "start = time.time()\n",
    "loss = np.zeros_like(x)\n",
    "for i in range(num_tree):\n",
    "    # get summary_tree first \n",
    "    if i==0:\n",
    "        res = y - np.mean(y)\n",
    "    else:\n",
    "        res = y - staged_predict[i-1]\n",
    "        \n",
    "    summary_tree = summarize_tree(ensemble_tree[i, 0].tree_)\n",
    "    explainer = shap.TreeExplainer(ensemble_tree[i, 0])\n",
    "    loss += loss_treeshap(x, res, summary_tree, explainer, alpha)\n",
    "\n",
    "\n",
    "#print(\"Loss sum + intercept: \" + str(np.sum(loss, axis=1) + (y - np.mean(y)) ** 2))\n",
    "#print(\"Real loss: \" + str((y - ypred) ** 2) + \"\\n\")\n",
    "\n",
    "#explainer.shap_values(x)\n",
    "\n",
    "#shap.TreeExplainer(tree_regressor).shap_values(x)\n",
    "\n",
    "\n",
    "ypred = tree_regressor.predict(x)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "sse = np.sum((y - ypred) ** 2)\n",
    "rsq = 1 - sse/sst\n",
    "\n",
    "sst = np.sum((y-np.mean(y)) ** 2)\n",
    "rsq_res = 0 - np.sum(loss, axis=0) / sst\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "#print(loss)\n",
    "print(np.round(rsq_res, 3))\n",
    "print(\"treeshap_rsq: \" + str(np.sum(rsq_res)))\n",
    "print(\"Model rsq: \"+ str(rsq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88d2b6",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5a6aa",
   "metadata": {},
   "source": [
    "## Formatter of xgboost since the data structure for xgboost is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf116798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015230178833007812\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import uuid\n",
    "import json\n",
    "#print(xgb_regressor.get_booster().get_dump())\n",
    "# We need \n",
    "# children_left: left children_index\n",
    "# children_right: right children_index\n",
    "# feature: array of features splitted at each node\n",
    "# threshold: array of thresholds for corresponding splitting features\n",
    "# max_depth: max_depth of the tree\n",
    "# n_node_samples: array of sample size for each node\n",
    "# value: array of values for each node, only leaf value is used, so only keep leaf value is fine\n",
    "# node_count: total number of nodes \n",
    "\n",
    "\n",
    "x, y, coefficients = make_regression(n_samples=1000, n_features=100, n_informative=3, coef=True, random_state=0)\n",
    "\n",
    "max_depth = 3\n",
    "\n",
    "#xgb_regressor = xgb.XGBRegressor(n_estimators = 100, max_depth=max_depth, eta=0.1, alpha=0, reg_lambda=0)\n",
    "xgb_regressor = xgboost.XGBRegressor(n_estimators = 100)\n",
    "xgb_regressor.fit(x, y)\n",
    "\n",
    "# dump\n",
    "#model_dump = xgb_regressor.get_booster().get_dump()\n",
    "#print(model_dump)\n",
    "#raw = xgb_regressor.get_booster().save_raw()\n",
    "#raw.read('f')\n",
    "\n",
    "#xgb_regressor.save_model(\"xgb_model_tmp.json\")\n",
    "unique_filename = str(uuid.uuid4())\n",
    "model_filename = f\"xgb_model_{unique_filename}.json\"\n",
    "xgb_regressor.save_model(model_filename)\n",
    "\n",
    "# import json\n",
    "# with open('xgb_model_tmp.json', 'r') as file:\n",
    "#     model_data = json.load(file)\n",
    "    \n",
    "#\n",
    "# Load the model data\n",
    "with open(model_filename, 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "\n",
    "# Delete it after loading \n",
    "os.remove(model_filename)\n",
    "\n",
    "def xgb_formatter(model_data, max_depth):\n",
    "    \"\"\"\n",
    "    This function takes the json format of the xgboost output and transform it to a list that treeshap rsq can understand\n",
    "    \n",
    "    Parameters:\n",
    "    model_data: json file\n",
    "    max_depth: the max tree depth\n",
    "    \n",
    "    Examples:\n",
    "    import json\n",
    "    xgb_regressor.save_model(\"model.json\")\n",
    "    with open('model.json', 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "    xgb_tree_res = xgb_formatter(model_data, 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    trees_data = model_data[\"learner\"][\"gradient_booster\"][\"model\"][\"trees\"]\n",
    "\n",
    "    xgb_tree = []\n",
    "\n",
    "    for tree in trees_data:\n",
    "        xgb_tree.append(simple_tree(np.array(tree[\"left_children\"]),\n",
    "                                    np.array(tree[\"right_children\"]), \n",
    "                                    np.array(tree[\"split_indices\"]),\n",
    "                                    np.array(tree[\"split_conditions\"]),\n",
    "                                    max_depth, \n",
    "                                    np.array(tree[\"sum_hessian\"]), \n",
    "                                    np.array(tree[\"base_weights\"]), \n",
    "                                    int(tree[\"tree_param\"][\"num_nodes\"])))\n",
    "\n",
    "    return(xgb_tree)\n",
    "        \n",
    "\n",
    "# be careful about the sum_hessian here, it may not be reliable other than squared loss: https://stackoverflow.com/questions/33520460/how-is-xgboost-cover-calculated\n",
    "#print(trees_data)\n",
    "# Dataframe\n",
    "# xgb_regressor.get_booster().trees_to_dataframe()\n",
    "start = time.time()\n",
    "xgb_res = xgb_formatter(model_data, max_depth)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0284fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "isinstance(xgb_regressor, xgboost.sklearn.XGBRegressor)\n",
    "print(xgb_regressor.get_params().get(\"max_depth\", 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f4ef2",
   "metadata": {},
   "source": [
    "## Testing, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae049bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n",
      "0.31776905059814453\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.03007301 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "treeshap_rsq: 0.030073006102343987\n",
      "Model rsq: 0.03007297220855598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "n = 5000\n",
    "x, y, coefficients = make_regression(n_samples=n, n_features=100, n_informative=5, coef=True, random_state=0)\n",
    "\n",
    "#sigma = np.random.normal(0, 2, size=n)\n",
    "#y = y + sigma\n",
    "max_depth = 1\n",
    "learning_rate = 0.05\n",
    "xgb_regressor = xgboost.XGBRegressor(n_estimators = 1, max_depth=max_depth, eta=learning_rate, alpha=0, reg_lambda=0)\n",
    "#xgb_regressor = xgboost.XGBRegressor(eta=eta)\n",
    "xgb_regressor.fit(x, y)\n",
    "print(\"Finish\")\n",
    "\n",
    "\n",
    "xgb_booster = xgb_regressor.get_booster()\n",
    "xgb_regressor.save_model(\"model.json\")\n",
    "\n",
    "import json\n",
    "with open('model.json', 'r') as file:\n",
    "    model_data = json.load(file)\n",
    "    \n",
    "xgb_res = xgb_formatter(model_data, max_depth)\n",
    "\n",
    "base_score = np.float64(model_data[\"learner\"][\"learner_model_param\"]['base_score'])\n",
    "\n",
    "num_tree = len(xgb_res)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"xgb\")\n",
    "\n",
    "start = time.time()\n",
    "loss = np.zeros_like(x)\n",
    "for i in range(num_tree):\n",
    "    # get summary_tree first \n",
    "    if i==0:\n",
    "        #res = y - np.mean(y)\n",
    "        res = y - base_score\n",
    "    else:\n",
    "        #  here is different\n",
    "        res = y - xgb_regressor.predict(x, iteration_range=(0, i))\n",
    "        \n",
    "    # here is different\n",
    "    summary_tree = summarize_tree(xgb_res[i])\n",
    "    # differet\n",
    "    explainer = shap.TreeExplainer(xgb_booster[i])\n",
    "    \n",
    "    # learning rate is different\n",
    "    loss += loss_treeshap(x, res, summary_tree, explainer, 1)\n",
    "\n",
    "#print(\"Loss sum + intercept: \" + str(np.sum(loss, axis=1) + (y - np.mean(y)) ** 2))\n",
    "#print(\"Real loss: \" + str((y - ypred) ** 2) + \"\\n\")\n",
    "\n",
    "#explainer.shap_values(x)\n",
    "\n",
    "#shap.TreeExplainer(tree_regressor).shap_values(x)\n",
    "\n",
    "# first_pred = xgb_regressor.predict(x, iteration_range=(0, 1))\n",
    "# second_pred = xgb_regressor.predict(x, iteration_range=(0, 2))\n",
    "# third_pred = xgb_regressor.predict(x, iteration_range=(0, 3))\n",
    "\n",
    "# final_pred =  xgb_regressor.predict(x)\n",
    "\n",
    "#before_first = xgb_regressor.predict(x, iteration_range=(0, 1)) - xgb_booster[0].predict(xgboost.DMatrix(x))\n",
    "#y_mean = np.mean(y)\n",
    "#print(before_first - y_mean)\n",
    "#print(before_first)\n",
    "\n",
    "\n",
    "#print(np.corrcoef(y, first_pred))\n",
    "# print(np.corrcoef(y, second_pred))\n",
    "# print(np.corrcoef(y, third_pred))\n",
    "# print(np.corrcoef(y, final_pred))\n",
    "\n",
    "ypred = xgb_regressor.predict(x)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "sse = np.sum((y - ypred) ** 2)\n",
    "rsq = 1 - sse/sst\n",
    "\n",
    "sst = np.sum((y-np.mean(y)) ** 2)\n",
    "rsq_res = 0 - np.sum(loss, axis=0) / sst\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "print(rsq_res)\n",
    "print(\"treeshap_rsq: \" + str(np.sum(rsq_res)))\n",
    "print(\"Model rsq: \"+ str(rsq) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
