# Sample testing code for LightGBM integration with Q-SHAP R package
# This script demonstrates how to use the Q-SHAP package with LightGBM models

library(lightgbm)
library(qshapr)

# ===============================
# Test 1: Basic LightGBM Model with California Housing Data
# ===============================

# Load sample data (using built-in dataset)
data(mtcars)
X <- as.matrix(mtcars[, -1])  # Features (excluding mpg)
y <- mtcars$mpg  # Target variable

# Split data into train and test
set.seed(42)
train_idx <- sample(1:nrow(X), 0.8 * nrow(X))
X_train <- X[train_idx, ]
y_train <- y[train_idx]
X_test <- X[-train_idx, ]
y_test <- y[-train_idx]

# Create LightGBM dataset
dtrain <- lgb.Dataset(data = X_train, label = y_train)

# Define parameters for regression
params <- list(
  objective = "regression",
  metric = "rmse",
  boosting_type = "gbdt",
  num_leaves = 31,
  learning_rate = 0.05,
  feature_fraction = 0.9,
  bagging_fraction = 0.8,
  bagging_freq = 5,
  verbose = -1
)

# Train LightGBM model
print("Training LightGBM model...")
lgb_model <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = -1
)

# Test basic prediction
pred_test <- predict(lgb_model, X_test)
rmse <- sqrt(mean((pred_test - y_test)^2))
print(paste("Test RMSE:", round(rmse, 4)))

# ===============================
# Test 2: Create Q-SHAP Tree Explainer for LightGBM
# ===============================

print("Creating Q-SHAP tree explainer...")
explainer <- create_tree_explainer(lgb_model)

# Verify explainer properties
print(paste("Model type:", explainer$model_type))
print(paste("Max depth:", explainer$max_depth))
print(paste("Number of trees:", length(explainer$trees)))

# ===============================
# Test 3: Calculate Q-SHAP Loss
# ===============================

print("Calculating Q-SHAP loss...")

# Use a small subset for faster computation
test_subset_idx <- 1:min(5, nrow(X_test))
X_test_subset <- X_test[test_subset_idx, , drop = FALSE]
y_test_subset <- y_test[test_subset_idx]

# Calculate Q-SHAP loss
loss <- qshap_loss(explainer, X_test_subset, y_test_subset)

print("Q-SHAP loss calculated successfully!")
print(paste("Loss matrix dimensions:", paste(dim(loss), collapse = " x ")))
print("Sample loss values:")
print(loss[1:min(3, nrow(loss)), 1:min(3, ncol(loss))])

# ===============================
# Test 4: Calculate Q-SHAP R-squared
# ===============================

print("Calculating Q-SHAP R-squared...")

# Calculate Shapley R-squared
rsq_result <- qshap_rsq(explainer, X_test_subset, y_test_subset, loss_out = TRUE)

print("Shapley R-squared values:")
print(rsq_result$rsq)

# ===============================
# Test 5: Compare with XGBoost (if available)
# ===============================

if (requireNamespace("xgboost", quietly = TRUE)) {
  print("Comparing with XGBoost...")
  
  # Train equivalent XGBoost model
  xgb_dtrain <- xgboost::xgb.DMatrix(data = X_train, label = y_train)
  
  xgb_params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    max_depth = 6,
    eta = 0.05,
    subsample = 0.8,
    colsample_bytree = 0.9
  )
  
  xgb_model <- xgboost::xgb.train(
    params = xgb_params,
    data = xgb_dtrain,
    nrounds = 100,
    verbose = 0
  )
  
  # Create XGBoost explainer
  xgb_explainer <- create_tree_explainer(xgb_model)
  
  # Calculate XGBoost Q-SHAP R-squared
  xgb_rsq_result <- qshap_rsq(xgb_explainer, X_test_subset, y_test_subset)
  
  print("LightGBM Q-SHAP R-squared:")
  print(rsq_result$rsq)
  print("XGBoost Q-SHAP R-squared:")
  print(xgb_rsq_result)
}

# ===============================
# Test 6: Edge Cases and Error Handling
# ===============================

print("Testing edge cases...")

# Test with single sample
single_x <- X_test[1, , drop = FALSE]
single_y <- y_test[1]

single_loss <- qshap_loss(explainer, single_x, single_y)
print(paste("Single sample loss calculated. Dimensions:", paste(dim(single_loss), collapse = " x ")))

# Test with different max_depth values
print("Testing different max_depth values...")
for (depth in c(3, 5, 10)) {
  print(paste("Testing with max_depth =", depth))
  
  # Train model with specific depth
  depth_params <- params
  depth_params$num_leaves <- min(2^depth, 31)  # Adjust num_leaves based on depth
  
  depth_model <- lgb.train(
    params = depth_params,
    data = dtrain,
    nrounds = 50,
    verbose = -1
  )
  
  depth_explainer <- create_tree_explainer(depth_model)
  depth_rsq <- qshap_rsq(depth_explainer, X_test_subset[1:2, , drop = FALSE], y_test_subset[1:2])
  
  print(paste("  Max depth:", depth_explainer$max_depth, 
              "| Mean R-squared:", round(mean(depth_rsq), 4)))
}

print("All LightGBM tests completed successfully!")

# ===============================
# Test 7: Performance Timing (Optional)
# ===============================

print("Performance timing test...")

# Time the Q-SHAP calculation
start_time <- Sys.time()
timing_rsq <- qshap_rsq(explainer, X_test_subset, y_test_subset)
end_time <- Sys.time()

print(paste("Q-SHAP calculation time for", nrow(X_test_subset), "samples:", 
            round(as.numeric(end_time - start_time, units = "secs"), 4), "seconds"))

print("LightGBM Q-SHAP integration test suite completed!")
